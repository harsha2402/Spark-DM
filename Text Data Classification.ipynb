{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of text data using Apache Spark and Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Article data into a dataframe and adding a column with corresponding label to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_files(folder, label):\n",
    "    files = []\n",
    "    for filename in os.listdir(folder):\n",
    "        crs = open(folder+\"/\"+filename,'r', encoding=\"utf8\").read()\n",
    "        new_str = re.sub('[^a-zA-Z0-9 ]', '', crs)\n",
    "        #print(filename)\\\n",
    "        lists.append([classes[label],new_str])\n",
    "    return files\n",
    "\n",
    "folders = [\n",
    "    '../data/business',\n",
    "    '../data/music',\n",
    "    '../data/politics',\n",
    "    '../data/sports',\n",
    "]\n",
    "\n",
    "classes = [\n",
    "    'business',\n",
    "    'music',\n",
    "    'politics',\n",
    "    'sports',\n",
    "]\n",
    "\n",
    "cols = ['my_label', 'my_data']\n",
    "lists = []\n",
    "label = 0\n",
    "\n",
    "for folder in folders:\n",
    "    files = load_files(folder,label)\n",
    "    label = label+1\n",
    "   \n",
    "\n",
    "df1 = pd.DataFrame(lists, columns=cols)\n",
    "\n",
    "df1\n",
    "\n",
    "df1.to_csv(\"dataFrame.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing all the libraries required for preprocessing and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pyspark\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import re\n",
    "from numpy import array\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to stop an already existing context if any.\n",
    "# don't run if u haven't initiated context already\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# making a context.\n",
    "sc =SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "data = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('dataFrame.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- my_label: string (nullable = true)\n",
      " |-- my_data: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# REGEX Tokenizer\n",
    "rt = RegexTokenizer(inputCol=\"my_data\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# Removal of stop words\n",
    "astps = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\",\"ssh\",\"httprequest\"] \n",
    "sr = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(astps)\n",
    "# Word Count\n",
    "cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "# tf idf\n",
    "hTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|my_label|             my_data|               words|            filtered|         rawFeatures|            features|label|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|business|IT takes both a s...|[it, takes, both,...|[it, takes, both,...|(10000,[1,5,7,14,...|(10000,[1,5,7,14,...|  0.0|\n",
      "|business| IF you ask for i...|[if, you, ask, fo...|[if, you, ask, fo...|(10000,[0,1,7,15,...|(10000,[0,1,7,15,...|  0.0|\n",
      "|business|NOT everyone is g...|[not, everyone, i...|[not, everyone, i...|(10000,[2,8,18,47...|(10000,[2,8,18,47...|  0.0|\n",
      "|business|You know you need...|[you, know, you, ...|[you, know, you, ...|(10000,[1,2,15,21...|(10000,[1,2,15,21...|  0.0|\n",
      "|business|PRO bono isnt jus...|[pro, bono, isnt,...|[pro, bono, isnt,...|(10000,[11,20,21,...|(10000,[11,20,21,...|  0.0|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating the pipe line\n",
    "label_stringIdx = StringIndexer(inputCol = \"my_label\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[rt, sr, hTF, idf, label_stringIdx])\n",
    "pFit = pipeline.fit(data)\n",
    "data1 = pFit.transform(data)\n",
    "data1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification 1: Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# splitting the training and test data into two sets\n",
    "# ratios are 80% to 20%\n",
    "(training_Data, test_Data) = data1.randomSplit([0.8, 0.2], seed = 100)\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(training_Data)\n",
    "predictions = lrModel.transform(test_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in (%) is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "90.91348265261308"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using MulticlassClassificationEvalutator to evalutate the accuracy of preditcions \n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"Accuracy in (%) is: \")\n",
    "evaluator.evaluate(predictions)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification 2: Naive Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predicting using NaiveBayesian classification\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "model = nb.fit(training_Data)\n",
    "predictions = model.transform(test_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in (%) is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "93.12529137529137"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluating the accuracy of NaiveBayesian classifier on test data\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"Accuracy in (%) is: \")\n",
    "evaluator.evaluate(predictions)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Unknown data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the unknown data and transforming it to fit our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_files2(folder, label1):\n",
    "    files = []\n",
    "    for filename in os.listdir(folder):\n",
    "        crs1 = open(folder+\"/\"+filename,'r', encoding=\"utf8\").read()\n",
    "        new_str1 = re.sub('[^a-zA-Z0-9 ]', ' ', crs1)\n",
    "        #print(filename)\\\n",
    "        lists1.append([classes1[label1],new_str1])\n",
    "    return files\n",
    "\n",
    "folders1 = [\n",
    "    '../unknown/business',\n",
    "    '../unknown/music',\n",
    "    '../unknown/politics',\n",
    "    '../unknown/sports',\n",
    "]\n",
    "\n",
    "classes1 = [\n",
    "    'business',\n",
    "    'music',\n",
    "    'politics',\n",
    "    'sports',\n",
    "]\n",
    "\n",
    "cols = ['my_label', 'my_data']\n",
    "lists1 = []\n",
    "label1 = 0\n",
    "\n",
    "for folder in folders1:\n",
    "    files = load_files2(folder,label1)\n",
    "    label1 = label1+1\n",
    "   \n",
    "\n",
    "df2 = pd.DataFrame(lists1, columns=cols)\n",
    "\n",
    "df2\n",
    "\n",
    "df2.to_csv(\"unknown.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unknownData = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('unknown.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unknownData1 = pFit.transform(unknownData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using Linear Regression for Unknown data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in (%) is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "90.57784320942216"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions1 = lrModel.transform(unknownData1)\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"Accuracy in (%) is: \")\n",
    "evaluator.evaluate(predictions1)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using Naive Bayesian classifier for Unknown data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in (%) is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "96.95781342840166"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions2 = model.transform(unknownData1)\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"Accuracy in (%) is: \")\n",
    "evaluator.evaluate(predictions2)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## http://spark.apache.org/docs/2.2.0/api/python/_modules/pyspark/ml/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## http://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://spark.apache.org/docs/2.1.0/ml-classification-regression.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
